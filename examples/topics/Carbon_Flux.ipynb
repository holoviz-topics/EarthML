{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carbon Monitoring Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import pandas as pd\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to visualize the data used in the carbon monitoring project [nee_data_fusion](https://github.com/greyNearing/nee_data_fusion/) using Python tools.\n",
    "\n",
    "The goals of this notebook:\n",
    "\n",
    "* examine the measurements from each site\n",
    "* generate some visualization or global model to predict one site from every other site.\n",
    "* generate and explain model idea\n",
    "\n",
    "To run this notebook, you will need `RSIF_2007_2016_05N_01L.mat` in the `examples` directory which you can download from https://gentinelab.eee.columbia.edu/content/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading FluxNet data ``extract_fluxnet.m``\n",
    "\n",
    "[FluxNet](http://fluxnet.fluxdata.org/) is a worldwide collection of sensor stations that record a number of local variables relating to atmospheric conditions, solar flux and soil moisture. The data in the [nee_data_fusion](https://github.com/greyNearing/nee_data_fusion/) repository is expressed as a collection of CSV files where the site names are expressed in the filenames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines functions to\n",
    "\n",
    "* read in the data from all sites\n",
    "* do some data munging (i.e., date parsing, `NaN` replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import intake\n",
    "\n",
    "cat = intake.open_catalog('../catalog.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = ['P_ERA', 'TA_ERA', 'PA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'WS_ERA',\n",
    "        'VPD_ERA', 'SWC_F_MDS_1', 'SWC_F_MDS_2', 'SWC_F_MDS_3',\n",
    "        'TS_F_MDS_1', 'TS_F_MDS_2', 'TS_F_MDS_3', 'TIMESTAMP']\n",
    "\n",
    "train = [*filter(lambda x: x!= 'TIMESTAMP', keep), 'DOY', 'site']\n",
    "\n",
    "def read_and_clean_file(s3_path, predict=\"NEE_CUT_USTAR50\"):\n",
    "    df = cat.fluxnet_daily(s3_path=s3_path).to_dask()\n",
    "    \n",
    "    for col in keep:\n",
    "        if col not in df.columns:\n",
    "            if 'SWC_F' in col or 'TS_F' in col:\n",
    "                df = df.assign(**{col: 0})\n",
    "    \n",
    "    if not (set(df.columns) >= set(keep)) or predict not in df.columns:\n",
    "        print(s3_path, 'is missing required columns')\n",
    "        return\n",
    "\n",
    "    df[keep + [predict]] = df[keep + [predict]].fillna(0)\n",
    "    df = df.assign(DOY=df.TIMESTAMP.dt.dayofyear)\n",
    "\n",
    "    X = df[train]\n",
    "    X = X.assign(y=df[predict])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a helper function to load and clean a particular data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and clean data\n",
    "This will take a few minutes if the data is not cached yet. First we will get a list of all the files on the s3 bucket, then we will iterate over those files and cache, read, and munge the data in each one. This is necessary since the columns in each file don't necessarily match the columns in the other files. Before we concatenate across sites, we need to do some cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto.s3.connection import S3Connection\n",
    "\n",
    "conn = S3Connection()\n",
    "bucket = conn.get_bucket('earth-data')\n",
    "s3_paths = [f.key for f in bucket.list('carbon_flux/nee_data_fusion/FLX')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for s3_path in s3_paths:\n",
    "    dd = read_and_clean_file(s3_path)\n",
    "    if dd is not None:\n",
    "        datasets.append(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = cat.fluxnet_metadata().read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data\n",
    "\n",
    "Once the data are loaded in, they can be concatenated across sites and joined with the metadata (lat, lon, vegetation) relating to each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dask.dataframe.concat(datasets).compute()\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the categorical `igbp` field (vegetation), we will create a matrix of dummies where each column corresponds to one of the `igbp` types, the rows correspond to observations and all the cells are filled with 0 or 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_metadata = pd.get_dummies(metadata, columns=['igbp'])\n",
    "onehot_metadata['igbp'] = metadata['igbp']\n",
    "onehot_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll merge this dummies + metadata matrix with our concatenated matrix from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(X, onehot_metadata, on='site')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the data\n",
    "To speed up the rest of the computations, we'll take a sample (10%) of the observations. We'll also remove some variables that we don't want to use in the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show = df.sample(frac=0.10)\n",
    "sites = pd.Categorical(show['site']).codes\n",
    "dropped = {}\n",
    "for col in ['DOY', 'site', 'igbp', 'lat', 'lon']:\n",
    "    dropped[col] = show[col].copy()\n",
    "    show.pop(col)\n",
    "        \n",
    "print(\"{} observations and {} variables\".format(*show.shape))\n",
    "print(\"Generating a prediction with these variables: \\n  {}\".format(\n",
    "    \"\\n  \".join(list(\n",
    "        show.columns\n",
    "    ))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables are sufficient to create the linear models at every site. However, the site information is hidden from the visualization algorithm.\n",
    "\n",
    "* Good sanity checks:\n",
    "    - latitude encoded some structure, longitude does not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Linear models work well *at one site* but this is confounded by\n",
    "\n",
    "* lat/lon\n",
    "* day of year\n",
    "* environment type\n",
    "\n",
    "We want to generate some visualization that accounts for these 4 variables and helps generate some understanding.\n",
    "\n",
    "That is, these observations lie on some manifold. We want to learn the structure of that manifold, and visualize each observation on that manifold.\n",
    "\n",
    "This work attempts to find similar observations - observations that have a similar structure between the independent variables (e.g., `P_ERA`) and dependent variables (the carbon flux measurement `y`).\n",
    "\n",
    "UMAP is a tool for this, and has firm mathematical grounding (plus, it's nice to use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reduct = umap.UMAP(verbose=True, n_epochs=None)#, n_neighbors=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduct.fit(show.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reduct.embedding_\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['lat', 'lon', 'igbp']\n",
    "s = pd.DataFrame(dropped)\n",
    "s['x0'] = embedding[:, 0]\n",
    "s['x1'] = embedding[:, 1]\n",
    "for col in cols:\n",
    "    if col in show:\n",
    "        s[col] = show[col]\n",
    "    else:\n",
    "        if not col in s:\n",
    "            print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore this manifold by coloring a scatter plot according to different variables that we believe should have structure in this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panel.interact import interact\n",
    "import colorcet as cc\n",
    "\n",
    "color_by_columns = ['lat', 'lon', 'DOY', 'site', 'igbp']\n",
    "\n",
    "@interact(color=color_by_columns)\n",
    "def create_figure(color='lat'):\n",
    "    opts = {'plot': {'color_index': color, 'show_legend': False,\n",
    "                     'width': 600, 'height': 600, 'colorbar': True,\n",
    "                     'tools': ['hover']},\n",
    "            'style': {'cmap': 'magma', 'legend': False}\n",
    "    }\n",
    "    if color == 'DOY':\n",
    "        opts['style']['cmap'] = cc.cm['cyclic_mrybm_35_75_c68']\n",
    "    if color == 'igbp':\n",
    "        opts['style']['cmap'] = 'Category20'\n",
    "        opts['plot']['legend_position'] ='right'\n",
    "        opts['plot']['show_legend'] = True\n",
    "    if color == 'site':\n",
    "        opts['style']['cmap'] = 'Category20'\n",
    "        opts['plot']['colorbar'] = False\n",
    "        opts['plot']['width'] = 525\n",
    "\n",
    "    chart = hv.Scatter(\n",
    "        s, kdims=['x0', 'x1'], vdims=[color, 'site'], extents=(-15,-15,15,15)\n",
    "    ).opts(**opts).relabel('Colored by: {}'.format(color))\n",
    "    return chart\n",
    "\n",
    "create_figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a closer look at vegetation\n",
    "\n",
    "We can specify a more custom color map for vegetation and rename the categories with more specific labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igbp_vegetation = {\n",
    "    'ENF': '01 - Evergreen Needleleaf forest',\n",
    "    'EBF': '02 - Evergreen Broadleaf forest',\n",
    "    'DNF': '03 - Deciduous Needleleaf forest',\n",
    "    'DBF': '04 - Deciduous Broadleaf forest',\n",
    "    'MF' : '05 - Mixed forest',\n",
    "    'CSH': '06 - Closed shrublands',\n",
    "    'OSH': '07 - Open shrublands',\n",
    "    'WSA': '08 - Woody savannas',\n",
    "    'SAV': '09 - Savannas',\n",
    "    'GRA': '10 - Grasslands',\n",
    "    'WET': '11 - Permanent wetlands',\n",
    "    'CRO': '12 - Croplands',\n",
    "}\n",
    "\n",
    "# https://lpdaac.usgs.gov/about/news_archive/modisterra_land_cover_types_yearly_l3_global_005deg_cmg_mod12c1\n",
    "lpdaac_palette = [\n",
    "    '#008000', '#00FF00', '#99CC00', '#99FF99', '#339966', '#993366',\n",
    "    '#FFCC99', '#CCFFCC', '#FFCC00', '#FF9900', '#006699', '#FFFF00'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Scatter [width=800, height=600] (color=Cycle(lpdaac_palette), size=1, muted_alpha=0)\n",
    "\n",
    "s['vegetation'] = s['igbp'].apply(lambda x: igbp_vegetation[x])\n",
    "ds = hv.Dataset(s, ['x0', 'vegetation'], ['x1', 'site'])\n",
    "grouped = ds.to(hv.Scatter, kdims=['x0', 'x1'], extents=(-15,-15,15,15), vdims=['site'])\n",
    "\n",
    "grouped.overlay('vegetation').options(legend_position='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate each vegetation type so that any site eccentricities are made clear. In this, let's **color by site ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.options(color_index='site', cmap='Category20', show_legend=False, size=1, alpha=0.8).layout().cols(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will train a model to predict carbon flux globally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dask\n",
    "With dask, we can distribute tasks over cores and do parallel computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Linear models work well *at one site* but this is confounded by\n",
    "\n",
    "* lat/lon\n",
    "* day of year\n",
    "* environment type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "assert 'site' not in show.columns\n",
    "y = show['y'].values\n",
    "X = pd.DataFrame({col: show[col].values \n",
    "                  for col in show.columns \n",
    "                  if col != 'y'})\n",
    "print(\"X.shape =\", X.shape)\n",
    "assert 'y' not in X.columns\n",
    "\n",
    "# transform data matrix so 0 mean, unit variance for each feature\n",
    "X = StandardScaler().fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fit_and_predict(X_train, y_train, X_test, nbrs=False):\n",
    "    if nbrs:\n",
    "        _nbrs = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(X_train)\n",
    "        distances, indices = _nbrs.kneighbors(X_test)\n",
    "    else:\n",
    "        indices = np.arange(len(X_train), dtype=int)\n",
    "    \n",
    "    X_train_filtered = X_train[indices.flat[:]] \n",
    "    y_train_filtered = y_train[indices.flat[:]] \n",
    "        \n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_filtered, y_train_filtered)\n",
    "    return model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_stats(train_idx, test_idx, X, y, doy=None, predict_each='season', nbrs=False):\n",
    "    start = datetime(2000, 1, 1)\n",
    "    end = start + timedelta(days=365)\n",
    "    \n",
    "    if predict_each == 'month':\n",
    "        get_time_id = lambda dt: dt.month\n",
    "    elif predict_each == 'year':\n",
    "        get_time_id = lambda dt: 1\n",
    "    elif predict_each == 'season':\n",
    "        seasons = {'spring': [3, 4, 5],\n",
    "                   'summer': [6, 7, 8],\n",
    "                   'fall': [9, 10, 11],\n",
    "                   'winter': [12, 1, 2]}\n",
    "        seasons = {month: season_id\n",
    "                   for season_id, months in enumerate(seasons.values())\n",
    "                   for month in months}\n",
    "        get_time_id = lambda dt: seasons[dt.month] \n",
    "    else:\n",
    "        msg = \"predict_each should be in {'year', 'month', 'season'}, got '{}'\"\n",
    "        raise ValueError(msg.format(predict_each))\n",
    "    \n",
    "    # from https://stackoverflow.com/questions/153584/how-to-iterate-over-a-timespan-after-days-hours-weeks-and-months-in-python\n",
    "    time_partitions = {(dt - start).days: get_time_id(dt)\n",
    "                       for dt in rrule.rrule(rrule.DAILY, dtstart=start, until=end)}\n",
    "    time_partitions[366] = max(time_partitions.values())\n",
    "    \n",
    "    test_days = doy[test_idx]\n",
    "    \n",
    "    preds = []\n",
    "    for time_partition in time_partitions.values():\n",
    "        if len(time_partitions.values()) > 1:\n",
    "            time_idx = [i for i, day in enumerate(doy) if time_partitions[day] == time_partition]\n",
    "            \n",
    "            # get the test set specific to this time instance\n",
    "            time_test_idx = np.intersect1d(test_idx, time_idx)\n",
    "        else:\n",
    "            time_test_idx = test_idx \n",
    "\n",
    "        if len(time_test_idx) == 0:\n",
    "            continue\n",
    "            \n",
    "        y_hat = fit_and_predict(X[train_idx], y[train_idx], X[time_test_idx], nbrs=nbrs)\n",
    "        y_test = y[time_test_idx]\n",
    "        preds += [{'predicted': y_hat,\n",
    "                   'actual': y_test,\n",
    "                   'time_partition': time_partition,\n",
    "                   'corrcoef': np.corrcoef(y_hat, y_test)[0][1]}]\n",
    "    actual = [p['actual'] for p in preds]\n",
    "    predicted = [p['predicted'] for p in preds]\n",
    "    actual = np.concatenate(actual).flat[:]\n",
    "    predicted = np.concatenate(predicted).flat[:]\n",
    "    return {'time_partitions': preds,\n",
    "            'actual': actual,\n",
    "            'predicted': predicted,\n",
    "            'corrcoef': np.corrcoef(actual, predicted)[0][1]}\n",
    "\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "sep = LeaveOneGroupOut()\n",
    "train_idx, test_idx = list(sep.split(X, y, sites))[0]\n",
    "_ = prediction_stats(train_idx, test_idx, X, y, doy=dropped['DOY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "sep = LeaveOneGroupOut()\n",
    "corrs = []\n",
    "\n",
    "futures = []\n",
    "n_splits = sep.get_n_splits(X, y, sites)\n",
    "X_future = client.scatter(X)\n",
    "y_future = client.scatter(y)\n",
    "doy_future = client.scatter(dropped['DOY'])\n",
    "for i, (train_index, test_index) in enumerate(sep.split(X, y, sites)):\n",
    "    futures += [{'site_id': i,\n",
    "                 'train_index': train_index,\n",
    "                 'test_index': test_index,\n",
    "                 'stats': client.submit(prediction_stats,\n",
    "                                        train_index,\n",
    "                                        test_index,\n",
    "                                        X_future,\n",
    "                                        y_future,\n",
    "                                        doy=doy_future)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.gather(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [{'site_id': result['site_id'], **result['stats']}\n",
    "       for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(out)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts VLine [show_legend=False] VLine (color='red')\n",
    "corrs = df.corrcoef[~np.isnan(df.corrcoef.values)]\n",
    "frequencies, edges = np.histogram(corrs, 20)\n",
    "\n",
    "c1 = hv.Histogram((frequencies, edges), extents=(-1, None, 1, None))\n",
    "c2 = hv.VLine(np.mean(corrs), label='mean')\n",
    "c3 = hv.VLine(np.median(corrs), label='median')\n",
    "c1 * c2# * c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(corrs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
