{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carbon Monitoring Project\n",
    "\n",
    "[FluxNet](http://fluxnet.fluxdata.org/) is a worldwide collection of sensor stations that record a number of local variables relating to atmospheric conditions, solar flux and soil moisture. This notebook aims to visualize the data used in the carbon monitoring project [nee_data_fusion](https://github.com/greyNearing/nee_data_fusion/) using Python tools.\n",
    "\n",
    "The goals of this notebook are to:\n",
    "\n",
    "* examine the carbon flux measurements from each site\n",
    "* determine the feasibility of using a model to predict the carbon flux at one site from every other site.\n",
    "* generate and explain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import pandas as pd\n",
    "\n",
    "import hvplot.pandas\n",
    "import geoviews.tile_sources as gts\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the `intake` catalog\n",
    "This notebook uses [`intake`](https://intake.readthedocs.io/) to set up a data catalog with instructions for loading data for various projects. Before we read in any data, we'll open that catalog file and inspect the various data sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "\n",
    "cat = intake.open_catalog('../catalog.yml')\n",
    "list(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata\n",
    "First we will load in the fluxnet_metadata containing some site information for each of the fluxnet sites. Included in these data are the lat and lon of each site and the vegetation encoding (more on this below). In the next cell we will read in these data and take a look at a random few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = cat.fluxnet_metadata().read()\n",
    "metadata.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vegetation type is classified according to the categories set out in the International Geosphereâ€“Biosphere Programme (**igbd**) with several additional categories defined on the [fluxdata website](http://www.fluxdata.org/DataInfo/Dataset%20Doc%20Lib/VegTypeIGBP.aspx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igbp_vegetation = {\n",
    "    'WAT': '00 - Water',\n",
    "    'ENF': '01 - Evergreen Needleleaf Forest',\n",
    "    'EBF': '02 - Evergreen Broadleaf Forest',\n",
    "    'DNF': '03 - Deciduous Needleleaf Forest',\n",
    "    'DBF': '04 - Deciduous Broadleaf Forest',\n",
    "    'MF' : '05 - Mixed Forest',\n",
    "    'CSH': '06 - Closed Shrublands',\n",
    "    'OSH': '07 - Open shrublands',\n",
    "    'WSA': '08 - Woody Savannas',\n",
    "    'SAV': '09 - Savannas',\n",
    "    'GRA': '10 - Grasslands',\n",
    "    'WET': '11 - Permanent Wetlands',\n",
    "    'CRO': '12 - Croplands',\n",
    "    'URB': '13 - Urban and Built-up',\n",
    "    'CNV': '14 - Cropland/Nartural Vegetation Mosaics',\n",
    "    'SNO': '15 - Snow and Ice',\n",
    "    'BSV': '16 - Baren or Sparsely Vegetated'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the dictionary above to map from igbp codes to longer labels - creating a new column on our metadata. We will make this column an ordered categorical to improve visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "dtype = CategoricalDtype(ordered=True, categories=sorted(igbp_vegetation.values()))\n",
    "metadata['vegetation'] = (metadata['igbp']\n",
    "                          .apply(lambda x: igbp_vegetation[x])\n",
    "                          .astype(dtype))\n",
    "metadata.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the fluxdata sites\n",
    "The pyviz ecosystem strives to make it always straightforward to visualize your data. Here we will use Open Street Map tiles from `geoviews` \n",
    "to make a quick map of where the different sites are located and the vegetation at each site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.hvplot.points('lon', 'lat', geo=True, color='vegetation',\n",
    "                       height=420, width=800, cmap='Category20') * gts.OSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading FluxNet data ``extract_fluxnet.m``\n",
    "\n",
    "The data in the [nee_data_fusion](https://github.com/greyNearing/nee_data_fusion/) repository is expressed as a collection of CSV files where the site names are expressed in the filenames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines a function to:\n",
    "\n",
    "* read in the data from all sites\n",
    "* discard columns that we don't need\n",
    "* calculate day of year\n",
    "\n",
    "And another one to print progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_columns = ['P_ERA', 'TA_ERA', 'PA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'WS_ERA',\n",
    "                     'VPD_ERA', 'TIMESTAMP', 'site', 'NEE_CUT_USTAR50']\n",
    "not_necessary_columns = ['SWC_F_MDS_1', 'SWC_F_MDS_2', 'SWC_F_MDS_3',\n",
    "                         'TS_F_MDS_1', 'TS_F_MDS_2', 'TS_F_MDS_3']\n",
    "\n",
    "keep_from_csv = necessary_columns + not_necessary_columns\n",
    "\n",
    "y = 'NEE_CUT_USTAR50'\n",
    "\n",
    "def season(df, metadata):\n",
    "    \"\"\"Add season column based on lat and month\n",
    "    \"\"\"\n",
    "    site = df['site'].cat.categories.item()\n",
    "    lat = metadata[metadata['site'] == site]['lat'].item()\n",
    "    if lat > 0:\n",
    "        seasons = {3: 'spring',  4: 'spring',  5: 'spring',\n",
    "                   6: 'summer',  7: 'summer',  8: 'summer',\n",
    "                   9: 'fall',   10: 'fall',   11: 'fall',\n",
    "                  12: 'winter',  1: 'winter',  2: 'winter'}\n",
    "    else:\n",
    "        seasons = {3: 'fall',    4: 'fall',    5: 'fall',\n",
    "                   6: 'winter',  7: 'winter',  8: 'winter',\n",
    "                   9: 'spring', 10: 'spring', 11: 'spring',\n",
    "                  12: 'summer',  1: 'summer',  2: 'summer'}\n",
    "    return df.assign(season=df.TIMESTAMP.dt.month.map(seasons))\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Clean data columns:\n",
    "    \n",
    "     * adds nan col for missing columns\n",
    "     * throws away un-needed columns\n",
    "     * adds day of year\n",
    "    \"\"\"\n",
    "    df = df.assign(**{col: np.nan for col in keep_from_csv if col not in df.columns})\n",
    "    df = df[keep_from_csv]\n",
    "    \n",
    "    df = df.assign(DOY=df.TIMESTAMP.dt.dayofyear)\n",
    "    df = df.assign(year=df.TIMESTAMP.dt.year)\n",
    "    df = season(df, metadata)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def print_progress(i, new_line_at=60):\n",
    "    \"\"\"Print a dot for each i creating a new line every `new_line_at`\n",
    "    \"\"\"\n",
    "    if (i + 1) % new_line_at != 0:\n",
    "        print('.', end='')\n",
    "    else: \n",
    "        print('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and clean data\n",
    "This will take a few minutes if the data is not cached yet. First we will get a list of all the files on the s3 bucket, then we will iterate over those files and cache, read, and munge the data in each one. This is necessary since the columns in each file don't necessarily match the columns in the other files. Before we concatenate across sites, we need to do some cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3fs import S3FileSystem\n",
    "s3 = S3FileSystem(anon=True)\n",
    "s3_paths = s3.glob('earth-data/carbon_flux/nee_data_fusion/FLX*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "skipped = []\n",
    "used = []\n",
    "\n",
    "for i, s3_path in enumerate(s3_paths):\n",
    "    print_progress(i)\n",
    "    \n",
    "    dd = cat.fluxnet_daily(s3_path=s3_path).to_dask()\n",
    "    site = dd['site'].cat.categories.item()\n",
    "    \n",
    "    if not set(dd.columns) >= set(necessary_columns):\n",
    "        skipped.append(site)\n",
    "        continue\n",
    "\n",
    "    datasets.append(clean_data(dd))\n",
    "    used.append(site)\n",
    "\n",
    "print()\n",
    "print('Found {} fluxnet sites with enough data to use - skipped {}'.format(len(used), len(skipped)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of datasets, we will concatenate across all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "X = dask.dataframe.concat(datasets).compute()\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also set the dtype of site to category. This will come in handy later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['site'] = X['site'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data Availability\n",
    "\n",
    "We can look at the sites for which we have data. We'll plot the sites on a world map again - this time using a custom colormap to denote sites with valid data, sites where data exist but were not loaded because too many fields were missing, and sites where no data was available. In addition to this map we'll get the count of different vegetation types at the sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(x):\n",
    "    if x in used:\n",
    "        return 'valid'\n",
    "    elif x in skipped:\n",
    "        return 'skipped'\n",
    "    else:\n",
    "        return 'no data'\n",
    "    \n",
    "cmap = {'valid': 'green', 'skipped': 'red', 'no data': 'gray'}\n",
    "\n",
    "QA = metadata.copy()\n",
    "QA['quality'] = QA['site'].map(mapper)\n",
    "\n",
    "\n",
    "world = QA.hvplot.points('lon', 'lat', geo=True, color='quality', cmap=cmap, hover_cols=['site', 'vegetation'],\n",
    "                         height=420, width=600).options(legend_position='top', tools=['hover', 'tap'])\n",
    "\n",
    "def veg_count(data):\n",
    "    veg_count = data['vegetation'].value_counts().sort_index(ascending=False)\n",
    "    return veg_count.hvplot.barh(height=420, width=500)\n",
    "\n",
    "hist = veg_count(QA[QA.quality=='valid']).relabel('Vegetation counts for valid sites')\n",
    "\n",
    "world * gts.OSM + hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a couple functions that generate plots on the full set of data or a subset of the data. We will use these in our dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_timeseries(data):\n",
    "    \"\"\"Make a timeseries plot showing the mean carbon flux at each DOY as well as the min and max\n",
    "    \"\"\"\n",
    "    return hv.Overlay([\n",
    "        data.groupby(['DOY', 'year'])[y].mean().groupby('DOY').agg([np.min, np.max]).hvplot.area('DOY', 'amin', 'amax', alpha=0.2, fields={'amin': y}),\n",
    "        data.groupby('DOY')[y].mean().hvplot()\n",
    "    ]).options(width=800)\n",
    "\n",
    "def one_count_plot(data):\n",
    "    \"\"\"Make a plot of the number of observations of each of the non-mandatory variables. \n",
    "    \"\"\"\n",
    "    return data[not_necessary_columns + ['site']].count().hvplot.bar(rot=90, width=300, height=300)\n",
    "\n",
    "timeseries = one_timeseries(X)\n",
    "count_plot = one_count_plot(X)\n",
    "timeseries + count_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard\n",
    "\n",
    "Using the plots and functions defined above, we can make a dashboard of sites where by clicking on a site, you get the timeseries and variable count for that particular site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews.streams import Selection1D\n",
    "import panel as pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = Selection1D(source=world)\n",
    "empty = timeseries.relabel('No selection') + count_plot.relabel('No selection')\n",
    "\n",
    "def on_select(index):\n",
    "    if not index:\n",
    "        return empty\n",
    "    i = index[0]\n",
    "    if i in QA[QA.quality=='valid'].index:\n",
    "        site = QA.iloc[i].site\n",
    "        ts = one_timeseries(X[X.site == site]).relabel(site)\n",
    "        ct = one_count_plot(X[X.site == site]).relabel(site)\n",
    "        return ts + ct\n",
    "    else:\n",
    "        return empty\n",
    "\n",
    "one_site = hv.DynamicMap(on_select, streams=[stream])\n",
    "\n",
    "pn.Column(pn.Row(world * gts.OSM, hist), pn.Row(one_site))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data\n",
    "\n",
    "Now that the data are loaded in we can merge the daily data with the metadata from before.\n",
    "\n",
    "In order to use the categorical `igbp` field, we will create a one hot encoding where each column corresponds to one of the `igbp` types, the rows correspond to observations and all the cells are filled with 0 or 1. This can be done use the method `pd.get_dummies`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_metadata = pd.get_dummies(metadata, columns=['igbp'])\n",
    "onehot_metadata.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll merge the metadata with all our daily observations - creating a tidy dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(X, onehot_metadata, on='site')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data Availability\n",
    "Now that all of our observations are merged with the site metadata, we can take a look at which sites have non-mandatory fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_some_extra_vars = df[df[not_necessary_columns].notnull().any(1)]\n",
    "have_some = metadata[metadata.site.isin(have_some_extra_vars.site.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_all_extra_vars = df[df[not_necessary_columns].notnull().all(1)]\n",
    "have_all = metadata[metadata.site.isin(have_all_extra_vars.site.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_some.hvplot.points('lon', 'lat', geo=True, hover_cols=['site', 'vegetation'], height=420, width=600).options(legend_position='top').relabel('have some extra vars') * \\\n",
    "have_all.hvplot.points('lon', 'lat', geo=True, hover_cols=['site', 'vegetation']).relabel('have all extra vars') * gts.OSM + \\\n",
    "veg_count(have_some) * veg_count(have_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there seems to be a strong geographic pattern in the availablity of soil moisture and soil temperature data, we won't use those columns in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=not_necessary_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set data to only the rows where there are no null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.notnull().all(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the data\n",
    "Now we need to split the data into columns that we'll use in the regression and columns that we'll use to explain our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_cols = ['DOY', 'lat', 'lon', 'season', 'site', 'vegetation', 'year']\n",
    "data_cols = ['P_ERA', 'TA_ERA', 'PA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'WS_ERA', 'VPD_ERA']\n",
    "igbp_cols = [col for col in df.columns if col.startswith('igbp')]\n",
    "regression_cols = data_cols + igbp_cols + [y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the rest of the computations, we'll take a sample (10%) of the observations. We'll also remove some variables that we don't want to use in the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(frac=0.10)\n",
    "\n",
    "explanatory_df = df_sample[explanatory_cols]\n",
    "regression_df = df_sample[regression_cols]\n",
    "regression_df = regression_df.rename(columns={y:'y'})\n",
    "\n",
    "print(\"{} observations and {} variables\".format(*regression_df.shape))\n",
    "print(\"Generating a prediction with these variables: \\n  {}\".format(\n",
    "    \"\\n  \".join(list(\n",
    "        regression_df.columns\n",
    "    ))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables are sufficient to create the linear models at every site. However, the site information is hidden from the visualization algorithm.\n",
    "\n",
    "* Good sanity checks:\n",
    "    - latitude encoded some structure, longitude does not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluxnet Data Analysis\n",
    "\n",
    "Linear models work well *at one site* but this is confounded by\n",
    "\n",
    "* lat/lon\n",
    "* day of year\n",
    "* environment type\n",
    "\n",
    "We want to generate some visualization that accounts for these 4 variables and helps generate some understanding.\n",
    "\n",
    "That is, these observations lie on some manifold. We want to learn the structure of that manifold, and visualize each observation on that manifold.\n",
    "\n",
    "This work attempts to find similar observations - observations that have a similar structure between the independent variables (e.g., `P_ERA`) and dependent variables (the carbon flux measurement `y`).\n",
    "\n",
    "UMAP is a tool for this, and has firm mathematical grounding (plus, it's nice to use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reduct = umap.UMAP(verbose=True, n_epochs=None)#, n_neighbors=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduct.fit(regression_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reduct.embedding_\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umapped_df = explanatory_df.join(pd.DataFrame(embedding, index=df_sample.index, columns=['x0', 'x1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore this manifold by coloring a scatter plot according to different variables that we believe should have structure in this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%opts Scatter [height=500 width=500, colorbar=True]\n",
    "\n",
    "color_by = 'lat'\n",
    "title = 'Observations colored by {}'.format(color_by)\n",
    "\n",
    "hv.Scatter(umapped_df, kdims=['x0', 'x1'], extents=(-15,-15,15,15)).relabel(title).options(color_index=color_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorcet as cc\n",
    "\n",
    "color_by = 'DOY'\n",
    "title = 'Observations colored by {}'.format(color_by)\n",
    "\n",
    "doy = hv.Scatter(umapped_df, kdims=['x0', 'x1'], extents=(-15,-15,15,15)).relabel(title)\\\n",
    "    .options(color_index=color_by, cmap=cc.cm['cyclic_mrybm_35_75_c68'])\n",
    "\n",
    "color_by = 'season'\n",
    "title = 'Observations colored by {}'.format(color_by)\n",
    "\n",
    "season = hv.Scatter(umapped_df, kdims=['x0', 'x1'], extents=(-15,-15,15,15)).relabel(title).options(color_index=color_by, cmap='Category10')\n",
    "\n",
    "doy + season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a closer look at vegetation\n",
    "\n",
    "We can specify a more custom color map for vegetation and rename the categories with more specific labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate each vegetation type so that any site eccentricities are made clear. In this, let's **color by site ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umapped_df.hvplot.scatter('x0', 'x1', by='vegetation', subplots=True, width=300, size=1, color='lat', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will train a model to predict carbon flux globally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dask\n",
    "With dask, we can distribute tasks over cores and do parallel computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df[regression_cols].values\n",
    "y = df['NEE_CUT_USTAR50'].values\n",
    "\n",
    "# transform data matrix so 0 mean, unit variance for each feature\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case - since the dataset is not too small, we'll use classical validation. Mention that you can use leave one out. Use the full dataset with classical validation rather than (10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = GroupShuffleSplit()\n",
    "train_idx, test_idx = next(sep.split(X, y, df.site.cat.codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sites = df.iloc[train_idx].site.unique()\n",
    "test_sites = df.iloc[test_idx].site.unique()\n",
    "\n",
    "train_site_metadata = metadata[metadata.site.isin(train_sites)]\n",
    "test_site_metadata = metadata[metadata.site.isin(test_sites)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a world map showing the sites that will be used as in training and those that will be used in testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_site_metadata.hvplot.points('lon', 'lat', geo=True, hover_cols=['site', 'vegetation'], height=420, width=600).options(legend_position='top').relabel('training sites') * \\\n",
    "test_site_metadata.hvplot.points('lon', 'lat', geo=True, hover_cols=['site', 'vegetation']).relabel('testing sites') * gts.OSM + \\\n",
    "veg_count(metadata[metadata.site.isin(train_sites)]) * veg_count(metadata[metadata.site.isin(test_sites)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fit_and_predict(X_train, y_train, X_test):\n",
    "    indices = np.arange(len(X_train), dtype=int)\n",
    "    \n",
    "    X_train_filtered = X_train[indices.flat[:]] \n",
    "    y_train_filtered = y_train[indices.flat[:]] \n",
    "        \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_filtered, y_train_filtered)\n",
    "    \n",
    "    return model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_stats(train_idx, test_idx, X, y, doy=None, predict_each='season'):\n",
    "    # Use the timestamp column and move this up.\n",
    "    start = datetime(2000, 1, 1)\n",
    "    end = start + timedelta(days=365)\n",
    "    \n",
    "    if predict_each == 'month':\n",
    "        get_time_id = lambda dt: dt.month\n",
    "    elif predict_each == 'year':\n",
    "        get_time_id = lambda dt: 1\n",
    "    elif predict_each == 'season':\n",
    "        # do this earlier and do a split as northern and southern hemisphere\n",
    "        seasons = {'spring': [3, 4, 5],\n",
    "                   'summer': [6, 7, 8],\n",
    "                   'fall': [9, 10, 11],\n",
    "                   'winter': [12, 1, 2]}\n",
    "        seasons = {month: season_id\n",
    "                   for season_id, months in enumerate(seasons.values())\n",
    "                   for month in months}\n",
    "        get_time_id = lambda dt: seasons[dt.month] \n",
    "    else:\n",
    "        msg = \"predict_each should be in {'year', 'month', 'season'}, got '{}'\"\n",
    "        raise ValueError(msg.format(predict_each))\n",
    "    \n",
    "    # from https://stackoverflow.com/questions/153584/how-to-iterate-over-a-timespan-after-days-hours-weeks-and-months-in-python\n",
    "    time_partitions = {(dt - start).days: get_time_id(dt)\n",
    "                       for dt in rrule.rrule(rrule.DAILY, dtstart=start, until=end)}\n",
    "    time_partitions[366] = max(time_partitions.values())\n",
    "    \n",
    "    test_days = doy[test_idx]\n",
    "    \n",
    "    preds = []\n",
    "    for time_partition in time_partitions.values():\n",
    "        if len(time_partitions.values()) > 1:\n",
    "            time_idx = [i for i, day in enumerate(doy) if time_partitions[day] == time_partition]\n",
    "            \n",
    "            # get the test set specific to this time instance\n",
    "            time_test_idx = np.intersect1d(test_idx, time_idx)\n",
    "        else:\n",
    "            time_test_idx = test_idx \n",
    "\n",
    "        if len(time_test_idx) == 0:\n",
    "            continue\n",
    "            \n",
    "        y_hat = fit_and_predict(X[train_idx], y[train_idx], X[time_test_idx])\n",
    "        y_test = y[time_test_idx]\n",
    "        preds += [{'predicted': y_hat,\n",
    "                   'actual': y_test,\n",
    "                   'time_partition': time_partition,\n",
    "                   'corrcoef': np.corrcoef(y_hat, y_test)[0][1]}]\n",
    "    actual = [p['actual'] for p in preds]\n",
    "    predicted = [p['predicted'] for p in preds]\n",
    "    actual = np.concatenate(actual).flat[:]\n",
    "    predicted = np.concatenate(predicted).flat[:]\n",
    "    return {'time_partitions': preds,\n",
    "            'actual': actual,\n",
    "            'predicted': predicted,\n",
    "            'corrcoef': np.corrcoef(actual, predicted)[0][1]}\n",
    "\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "sep = LeaveOneGroupOut()\n",
    "train_idx, test_idx = list(sep.split(X, y, sites))[0]\n",
    "_ = prediction_stats(train_idx, test_idx, X, y, doy=dropped['DOY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "sep = LeaveOneGroupOut()\n",
    "corrs = []\n",
    "\n",
    "futures = []\n",
    "n_splits = sep.get_n_splits(X, y, sites)\n",
    "X_future = client.scatter(X)\n",
    "y_future = client.scatter(y)\n",
    "doy_future = client.scatter(dropped['DOY'])\n",
    "for i, (train_index, test_index) in enumerate(sep.split(X, y, sites)):\n",
    "    futures += [{'site_id': i,\n",
    "                 'train_index': train_index,\n",
    "                 'test_index': test_index,\n",
    "                 'stats': client.submit(prediction_stats,\n",
    "                                        train_index,\n",
    "                                        test_index,\n",
    "                                        X_future,\n",
    "                                        y_future,\n",
    "                                        doy=doy_future)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.gather(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [{'site_id': result['site_id'], **result['stats']}\n",
    "       for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(out)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts VLine [show_legend=False] VLine (color='red')\n",
    "corrs = df.corrcoef[~np.isnan(df.corrcoef.values)]\n",
    "frequencies, edges = np.histogram(corrs, 20)\n",
    "\n",
    "c1 = hv.Histogram((frequencies, edges), extents=(-1, None, 1, None))\n",
    "c2 = hv.VLine(np.mean(corrs), label='mean')\n",
    "c3 = hv.VLine(np.median(corrs), label='median')\n",
    "c1 * c2# * c3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance on the training data and then do it on the test data. Can we predict certain vegetations better than others. Color by correlation and then hover to see actual and predicted timeseries. \n",
    "\n",
    "fraction of explained variance.  Timeseries of actual and predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(corrs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
