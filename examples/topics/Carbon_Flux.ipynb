{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carbon Monitoring Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FluxNet](http://fluxnet.fluxdata.org/) is a worldwide collection of sensor stations that record a number of local variables relating to atmospheric conditions, solar flux and soil moisture. This notebook aims to visualize the data used in the carbon monitoring project [nee_data_fusion](https://github.com/greyNearing/nee_data_fusion/) using Python tools.\n",
    "\n",
    "The goals of this notebook are to:\n",
    "\n",
    "* examine the carbon flux measurements from each site\n",
    "* determine the feasibility of using a model to predict the carbon flux at one site from every other site.\n",
    "* generate and explain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import pandas as pd\n",
    "\n",
    "import hvplot.pandas\n",
    "import geoviews.tile_sources as gts\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the `intake` catalog\n",
    "This notebook uses [`intake`](https://intake.readthedocs.io/) to set up a data catalog with instructions for loading data for various projects. Before we read in any data, we'll open that catalog file and inspect the various data sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "\n",
    "cat = intake.open_catalog('../catalog.yml')\n",
    "list(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata\n",
    "First we will load in the fluxnet_metadata containing some site information for each of the fluxnet sites. Included in these data are the lat and lon of each site and the vegetation encoding (more on this below). In the next cell we will read in these data and take a look at a random few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = cat.fluxnet_metadata().read()\n",
    "metadata.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vegetation type is classified according to the categories set out in the International Geosphereâ€“Biosphere Programme (**igbd**) with several additional categories defined on the [fluxdata website](http://www.fluxdata.org/DataInfo/Dataset%20Doc%20Lib/VegTypeIGBP.aspx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igbp_vegetation = {\n",
    "    'WAT': '00 - Water',\n",
    "    'ENF': '01 - Evergreen Needleleaf Forest',\n",
    "    'EBF': '02 - Evergreen Broadleaf Forest',\n",
    "    'DNF': '03 - Deciduous Needleleaf Forest',\n",
    "    'DBF': '04 - Deciduous Broadleaf Forest',\n",
    "    'MF' : '05 - Mixed Forest',\n",
    "    'CSH': '06 - Closed Shrublands',\n",
    "    'OSH': '07 - Open shrublands',\n",
    "    'WSA': '08 - Woody Savannas',\n",
    "    'SAV': '09 - Savannas',\n",
    "    'GRA': '10 - Grasslands',\n",
    "    'WET': '11 - Permanent Wetlands',\n",
    "    'CRO': '12 - Croplands',\n",
    "    'URB': '13 - Urban and Built-up',\n",
    "    'CNV': '14 - Cropland/Nartural Vegetation Mosaics',\n",
    "    'SNO': '15 - Snow and Ice',\n",
    "    'BSV': '16 - Baren or Sparsely Vegetated'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the dictionary above to map from igbp codes to longer labels - creating a new column on our metadata. We will make this column an ordered categorical to improve visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "dtype = CategoricalDtype(ordered=True, categories=sorted(igbp_vegetation.values()))\n",
    "metadata['vegetation'] = (metadata['igbp']\n",
    "                          .apply(lambda x: igbp_vegetation[x])\n",
    "                          .astype(dtype))\n",
    "metadata.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the fluxdata sites\n",
    "The pyviz ecosystem strives to make it always straightforward to visualize your data. Here we will use Open Street Map tiles from `geoviews` \n",
    "to make a quick map of where the different sites are located and the vegetation at each site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.hvplot.points('lon', 'lat', geo=True, color='vegetation',\n",
    "                       height=420, width=800, cmap='Category20') * gts.OSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading FluxNet data ``extract_fluxnet.m``\n",
    "\n",
    "The data in the [nee_data_fusion](https://github.com/greyNearing/nee_data_fusion/) repository is expressed as a collection of CSV files where the site names are expressed in the filenames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines a function to:\n",
    "\n",
    "* read in the data from all sites\n",
    "* discard columns that we don't need\n",
    "* calculate day of year\n",
    "\n",
    "And another one to print progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_columns = ['P_ERA', 'TA_ERA', 'PA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'WS_ERA',\n",
    "                     'VPD_ERA', 'TIMESTAMP', 'site', 'NEE_CUT_USTAR50']\n",
    "not_necessary_columns = ['SWC_F_MDS_1', 'SWC_F_MDS_2', 'SWC_F_MDS_3',\n",
    "                         'TS_F_MDS_1', 'TS_F_MDS_2', 'TS_F_MDS_3']\n",
    "\n",
    "keep_from_csv = necessary_columns + not_necessary_columns\n",
    "\n",
    "y_variable = 'NEE_CUT_USTAR50'\n",
    "\n",
    "def season(df, metadata):\n",
    "    \"\"\"Add season column based on lat and month\n",
    "    \"\"\"\n",
    "    site = df['site'].cat.categories.item()\n",
    "    lat = metadata[metadata['site'] == site]['lat'].item()\n",
    "    if lat > 0:\n",
    "        seasons = {3: 'spring',  4: 'spring',  5: 'spring',\n",
    "                   6: 'summer',  7: 'summer',  8: 'summer',\n",
    "                   9: 'fall',   10: 'fall',   11: 'fall',\n",
    "                  12: 'winter',  1: 'winter',  2: 'winter'}\n",
    "    else:\n",
    "        seasons = {3: 'fall',    4: 'fall',    5: 'fall',\n",
    "                   6: 'winter',  7: 'winter',  8: 'winter',\n",
    "                   9: 'spring', 10: 'spring', 11: 'spring',\n",
    "                  12: 'summer',  1: 'summer',  2: 'summer'}\n",
    "    return df.assign(season=df.TIMESTAMP.dt.month.map(seasons))\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Clean data columns:\n",
    "    \n",
    "     * adds nan col for missing columns\n",
    "     * throws away un-needed columns\n",
    "     * adds day of year\n",
    "    \"\"\"\n",
    "    df = df.assign(**{col: np.nan for col in keep_from_csv if col not in df.columns})\n",
    "    df = df[keep_from_csv]\n",
    "    \n",
    "    df = df.assign(DOY=df.TIMESTAMP.dt.dayofyear)\n",
    "    df = df.assign(year=df.TIMESTAMP.dt.year)\n",
    "    df = season(df, metadata)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def print_progress(i, new_line_at=60):\n",
    "    \"\"\"Print a dot for each i creating a new line every `new_line_at`\n",
    "    \"\"\"\n",
    "    if (i + 1) % new_line_at != 0:\n",
    "        print('.', end='')\n",
    "    else: \n",
    "        print('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and clean data\n",
    "This will take a few minutes if the data is not cached yet. First we will get a list of all the files on the s3 bucket, then we will iterate over those files and cache, read, and munge the data in each one. This is necessary since the columns in each file don't necessarily match the columns in the other files. Before we concatenate across sites, we need to do some cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3fs import S3FileSystem\n",
    "s3 = S3FileSystem(anon=True)\n",
    "s3_paths = s3.glob('earth-data/carbon_flux/nee_data_fusion/FLX*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "skipped = []\n",
    "used = []\n",
    "\n",
    "for i, s3_path in enumerate(s3_paths):\n",
    "    print_progress(i)\n",
    "    \n",
    "    dd = cat.fluxnet_daily(s3_path=s3_path).to_dask()\n",
    "    site = dd['site'].cat.categories.item()\n",
    "    \n",
    "    if not set(dd.columns) >= set(necessary_columns):\n",
    "        skipped.append(site)\n",
    "        continue\n",
    "\n",
    "    datasets.append(clean_data(dd))\n",
    "    used.append(site)\n",
    "\n",
    "print()\n",
    "print('Found {} fluxnet sites with enough data to use - skipped {}'.format(len(used), len(skipped)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of datasets, we will concatenate across all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "X = dask.dataframe.concat(datasets).compute()\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also set the dtype of site to category. This will come in handy later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['site'] = X['site'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data Availability\n",
    "\n",
    "We can look at the sites for which we have data. We'll plot the sites on a world map again - this time using a custom colormap to denote sites with valid data, sites where data exist but were not loaded because too many fields were missing, and sites where no data was available. In addition to this map we'll get the count of different vegetation types at the sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(x):\n",
    "    if x in used:\n",
    "        return 'valid'\n",
    "    elif x in skipped:\n",
    "        return 'skipped'\n",
    "    else:\n",
    "        return 'no data'\n",
    "    \n",
    "cmap = {'valid': 'green', 'skipped': 'red', 'no data': 'gray'}\n",
    "\n",
    "QA = metadata.copy()\n",
    "QA['quality'] = QA['site'].map(mapper)\n",
    "\n",
    "\n",
    "world = QA.hvplot.points('lon', 'lat', geo=True, color='quality', cmap=cmap, hover_cols=['site', 'vegetation'],\n",
    "                         height=420, width=600).options(legend_position='top', tools=['hover', 'tap'])\n",
    "\n",
    "def veg_count(data):\n",
    "    veg_count = data['vegetation'].value_counts().sort_index(ascending=False)\n",
    "    return veg_count.hvplot.barh(height=420, width=500)\n",
    "\n",
    "hist = veg_count(QA[QA.quality=='valid']).relabel('Vegetation counts for valid sites')\n",
    "\n",
    "world * gts.OSM + hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a couple functions that generate plots on the full set of data or a subset of the data. We will use these in our dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_timeseries(data):\n",
    "    \"\"\"Make a timeseries plot showing the mean carbon flux at each DOY as well as the min and max\n",
    "    \"\"\"\n",
    "    return hv.Overlay([\n",
    "        (data.groupby(['DOY', 'year'])[y_variable]\n",
    "             .mean().groupby('DOY').agg([np.min, np.max])\n",
    "             .hvplot.area('DOY', 'amin', 'amax', alpha=0.2, fields={'amin': y_variable})),\n",
    "        data.groupby('DOY')[y_variable].mean().hvplot()\n",
    "    ]).options(width=800)\n",
    "\n",
    "def one_count_plot(data):\n",
    "    \"\"\"Make a plot of the number of observations of each of the non-mandatory variables. \n",
    "    \"\"\"\n",
    "    return data[not_necessary_columns + ['site']].count().hvplot.bar(rot=90, width=300, height=300)\n",
    "\n",
    "timeseries = one_timeseries(X)\n",
    "count_plot = one_count_plot(X)\n",
    "timeseries + count_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard\n",
    "\n",
    "Using the plots and functions defined above, we can make a dashboard of sites where by clicking on a site, you get the timeseries and variable count for that particular site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews.streams import Selection1D\n",
    "import panel as pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = Selection1D(source=world)\n",
    "empty = timeseries.relabel('No selection') + count_plot.relabel('No selection')\n",
    "\n",
    "def on_select(index):\n",
    "    if not index:\n",
    "        return empty\n",
    "    i = index[0]\n",
    "    if i in QA[QA.quality=='valid'].index:\n",
    "        site = QA.iloc[i].site\n",
    "        ts = one_timeseries(X[X.site == site]).relabel(site)\n",
    "        ct = one_count_plot(X[X.site == site]).relabel(site)\n",
    "        return ts + ct\n",
    "    else:\n",
    "        return empty\n",
    "\n",
    "one_site = hv.DynamicMap(on_select, streams=[stream])\n",
    "\n",
    "pn.Column(pn.Row(world * gts.OSM, hist), pn.Row(one_site))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data\n",
    "\n",
    "Now that the data are loaded in we can merge the daily data with the metadata from before.\n",
    "\n",
    "In order to use the categorical `igbp` field, we will create a one hot encoding where each column corresponds to one of the `igbp` types, the rows correspond to observations and all the cells are filled with 0 or 1. This can be done use the method `pd.get_dummies`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_metadata = pd.get_dummies(metadata, columns=['igbp'])\n",
    "onehot_metadata.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll merge the metadata with all our daily observations - creating a tidy dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(X, onehot_metadata, on='site')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data Availability\n",
    "Now that all of our observations are merged with the site metadata, we can take a look at which sites have non-mandatory fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_some_extra_vars = df[df[not_necessary_columns].notnull().any(1)]\n",
    "have_some = metadata[metadata.site.isin(have_some_extra_vars.site.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_all_extra_vars = df[df[not_necessary_columns].notnull().all(1)]\n",
    "have_all = metadata[metadata.site.isin(have_all_extra_vars.site.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_some.hvplot.points('lon', 'lat', geo=True, hover_cols=['site', 'vegetation'], height=420, width=600).options(legend_position='top').relabel('have some extra vars') * \\\n",
    "have_all.hvplot.points('lon', 'lat', geo=True, hover_cols=['site', 'vegetation']).relabel('have all extra vars') * gts.OSM + \\\n",
    "veg_count(have_some) * veg_count(have_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there seems to be a strong geographic pattern in the availablity of soil moisture and soil temperature data, we won't use those columns in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=not_necessary_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set data to only the rows where there are no null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.notnull().all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['site'] = df['site'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the data\n",
    "Now we need to split the data into columns that we'll use in the regression and columns that we'll use to explain our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_cols = ['DOY', 'lat', 'lon', 'season', 'site', 'vegetation', 'year']\n",
    "data_cols = ['P_ERA', 'TA_ERA', 'PA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'WS_ERA', 'VPD_ERA']\n",
    "igbp_cols = [col for col in df.columns if col.startswith('igbp')]\n",
    "regression_cols = data_cols + igbp_cols + [y_variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the rest of the computations, we'll take a sample (10%) of the observations. We'll also remove some variables that we don't want to use in the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(frac=0.10)\n",
    "\n",
    "explanatory_df = df_sample[explanatory_cols]\n",
    "regression_df = df_sample[regression_cols]\n",
    "regression_df = regression_df.rename(columns={y_variable:'y'})\n",
    "\n",
    "print(\"{} observations and {} variables\".format(*regression_df.shape))\n",
    "print(\"Generating a prediction with these variables: \\n  {}\".format(\n",
    "    \"\\n  \".join(list(\n",
    "        regression_df.columns\n",
    "    ))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these variables to predict y. Note the exclusion of lat/lon and day of year from the variables that we are using. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluxnet Structure Exploration\n",
    "\n",
    "Linear models work well *at one site* but this is confounded by:\n",
    "\n",
    "* lat/lon\n",
    "* day of year\n",
    "* environment type\n",
    "\n",
    "We want to explore the relationship between the linear model an these site identifiers. So basically if we are able to reduce the dimensionality of the variables and still see a relationship between the reduced dimensions and these site characteristics, then we stand a good chance of being able to create a reasonable model. We'll use [umap](https://github.com/lmcinnes/umap) - Uniform Manifold Approximation and Projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reduct = umap.UMAP(verbose=True, n_epochs=None)#, n_neighbors=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduct.fit(regression_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reduct.embedding_\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umapped_df = explanatory_df.join(pd.DataFrame(embedding, index=df_sample.index, columns=['x0', 'x1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore these reduced dimensions by coloring a scatter plot according to different variables that we believe should have structure in this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews.operation.datashader import rasterize, datashade\n",
    "from datashader import transfer_functions as tf, reductions as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that `lat` will be strongly correlated with the new dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by = 'lat'\n",
    "title = 'Observations colored by {}'.format(by)\n",
    "\n",
    "scat = hv.Scatter(umapped_df, kdims=['x0', 'x1'])\n",
    "p = scat.options(color_index=by, alpha=0.2, cmap='viridis', height=500, width=500, colorbar=True).relabel(title)\n",
    "\n",
    "p + rasterize(p, x_sampling=1, y_sampling=1, aggregator=rd.std(by)).relabel('Aggregated by standard deviation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that `lon` on the other hand will not show a strong relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by = 'lon'\n",
    "title = 'Observations colored by {}'\n",
    "\n",
    "p = scat.options(color_index=by, cmap='viridis',colorbar=True, \n",
    "                 alpha=0.2, size=1, width=350).relabel(title.format(by))\n",
    "\n",
    "p + rasterize(p, x_sampling=1, y_sampling=1, aggregator=rd.std(by))\\\n",
    "        .options(colorbar=True, width=350).relabel('Aggregated by standard deviation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day or year should show a relationship. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorcet as cc\n",
    "\n",
    "by = 'DOY'\n",
    "\n",
    "scat.options(color_index=by, cmap=cc.cm['cyclic_mrybm_35_75_c68'], \n",
    "             alpha=0.2, height=500, width=550, colorbar=True).relabel(title.format(by))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly season should show an even stronger relationship since we used lat and day of year to set it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by = 'season'\n",
    "title = 'Observation counts grouped by {}'\n",
    "\n",
    "datashade(scat.groupby(by), x_sampling=1, y_sampling=1).layout().relabel(title.format(by)).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a closer look at vegetation\n",
    "\n",
    "Since we included vegetation in our variables, we would expect the relationship between it and the reduced dimensions to be strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by = 'vegetation'\n",
    "datashade(scat.groupby(by), x_sampling=1, y_sampling=1).layout().relabel(title.format(by)).cols(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will train a model to predict carbon flux globally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dask\n",
    "With dask, we can distribute tasks over cores and do parallel computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case - since the dataset is not too small, we'll use classical validation. We can use the full dataset with classical validation rather than (10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df[data_cols + igbp_cols + ['lat', 'DOY']].values\n",
    "y = df[y_variable].values\n",
    "season = df['season'].values\n",
    "sites = df.site.cat.codes\n",
    "\n",
    "# transform data matrix so 0 mean, unit variance for each feature\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll shuffle the sites and select 10% of them to be used as a test set. The rest we will use for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "sep = GroupShuffleSplit(train_size=0.95, test_size=0.05)\n",
    "train_idx, test_idx = next(sep.split(X, y, sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sites = df.iloc[train_idx].site.unique()\n",
    "test_sites = df.iloc[test_idx].site.unique()\n",
    "\n",
    "train_site_metadata = metadata[metadata.site.isin(train_sites)]\n",
    "test_site_metadata = metadata[metadata.site.isin(test_sites)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a world map showing the sites that will be used as in training and those that will be used in testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_site_metadata.hvplot.points('lon', 'lat', geo=True, \n",
    "                                  hover_cols=['site', 'vegetation']\n",
    "                                  , height=420, width=600).options(legend_position='top').relabel('training sites') * \\\n",
    "test_site_metadata.hvplot.points('lon', 'lat', geo=True, \n",
    "                                 hover_cols=['site', 'vegetation']).relabel('testing sites') * gts.OSM + \\\n",
    "veg_count(metadata[metadata.site.isin(train_sites)]) * veg_count(metadata[metadata.site.isin(test_sites)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll construct a linear regression model using our randomly selected training sites and test sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X[train_idx], y[train_idx]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = model.predict(X[train_idx])\n",
    "np.corrcoef(y_hat_train, y[train_idx])[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = model.predict(X[test_idx])\n",
    "np.corrcoef(y_hat_test, y[test_idx])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datashade(hv.Scatter((y_hat_test, y[test_idx])), y_sampling=0.2, x_sampling=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_stats(train_idx, test_idx, X, y, season):\n",
    "    pred = {}\n",
    "    model = LinearRegression()\n",
    "    model.fit(X[train_idx], y[train_idx])\n",
    "    \n",
    "    for s in ['summer', 'fall', 'spring', 'winter']:\n",
    "        season_test_idx = np.where(season==s)\n",
    "\n",
    "        y_hat = model.predict(X[season_test_idx])\n",
    "        y_test = y[season_test_idx]\n",
    "        pred[s] = {'predicted': y_hat,\n",
    "                   'actual': y_test,\n",
    "                   'corrcoef': np.corrcoef(y_hat, y_test)[0][1]}\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll scatter our data using `dask` and make a bunch of different splits. For each split we'll compute the predicton stats for each season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = []\n",
    "sep = GroupShuffleSplit(n_splits=10, train_size=0.95, test_size=0.05)\n",
    "\n",
    "X_future = client.scatter(X)\n",
    "y_future = client.scatter(y)\n",
    "season_future = client.scatter(season)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(sep.split(X, y, sites)):\n",
    "    futures += [client.submit(prediction_stats, train_index, test_index,\n",
    "                              X_future, y_future, season_future)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our computations set up in dask, we can gather the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.gather(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And consolidate the results for each season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {'summer': {}, 'fall': {}, 'spring': {}, 'winter': {}}\n",
    "for s in output.keys():\n",
    "    for var in ['predicted', 'actual']:\n",
    "        output[s][var] = np.concatenate([i[s][var] for i in results if i[s]['corrcoef'] > 0.01])\n",
    "    output[s]['corrcoef'] = np.array([i[s]['corrcoef'] for i in results if i[s]['corrcoef'] > 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Layout([\n",
    "    datashade(\n",
    "        hv.Scatter((output[s]['predicted'], output[s]['actual'])),\n",
    "        y_sampling=0.2, x_sampling=0.2).relabel(s)\n",
    "    for s in ['summer', 'fall', 'spring', 'winter']]).cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = pd.DataFrame({s: output[s]['corrcoef'] for s in ['summer', 'fall', 'spring', 'winter']})\n",
    "\n",
    "corrs.T.hvplot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    " - Can we predict certain vegetations better than others. \n",
    " - Color by correlation and then hover to see actual and predicted timeseries. \n",
    " - fraction of explained variance.\n",
    " - Timeseries of actual and predicted.\n",
    " - include vegetation, lat, and season in test input"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
