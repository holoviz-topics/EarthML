{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carbon Monitoring Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import pandas as pd\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to use Python tools to predict global carbon flux. To accomplish this we'll use a model based on the data collected in the carbon monitoring project [nee_data_fusion](https://github.com/greyNearing/nee_data_fusion/).\n",
    "\n",
    "The goals of this notebook:\n",
    "\n",
    "* generate and explain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading FluxNet data ``extract_fluxnet.m``\n",
    "\n",
    "[FluxNet](http://fluxnet.fluxdata.org/) is a worldwide collection of sensor stations that record a number of local variables relating to atmospheric conditions, solar flux and soil moisture. The data in the [nee_data_fusion](https://github.com/greyNearing/nee_data_fusion/) repository is expressed as a collection of CSV files where the site names are expressed in the filenames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines functions to\n",
    "\n",
    "* read in the data from all sites\n",
    "* do some data munging (i.e., date parsing, `NaN` replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import intake\n",
    "\n",
    "cat = intake.open_catalog('../catalog.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = ['P_ERA', 'TA_ERA', 'PA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'WS_ERA',\n",
    "        'VPD_ERA', 'SWC_F_MDS_1', 'SWC_F_MDS_2', 'SWC_F_MDS_3',\n",
    "        'TS_F_MDS_1', 'TS_F_MDS_2', 'TS_F_MDS_3', 'TIMESTAMP']\n",
    "\n",
    "train = [*filter(lambda x: x!= 'TIMESTAMP', keep), 'DOY', 'site']\n",
    "\n",
    "def read_and_clean_file(s3_path, predict=\"NEE_CUT_USTAR50\"):\n",
    "    df = cat.fluxnet_daily(s3_path=s3_path).to_dask()\n",
    "    \n",
    "    for col in keep:\n",
    "        if col not in df.columns:\n",
    "            if 'SWC_F' in col or 'TS_F' in col:\n",
    "                df = df.assign(**{col: 0})\n",
    "    \n",
    "    if not (set(df.columns) >= set(keep)) or predict not in df.columns:\n",
    "        print(s3_path, 'is missing required columns')\n",
    "        return\n",
    "\n",
    "    df[keep + [predict]] = df[keep + [predict]].fillna(0)\n",
    "    df = df.assign(DOY=df.TIMESTAMP.dt.dayofyear)\n",
    "\n",
    "    X = df[train]\n",
    "    X = X.assign(y=df[predict])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a helper function to load and clean a particular data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and clean data\n",
    "This will take a few minutes if the data is not cached yet. First we will get a list of all the files on the s3 bucket, then we will iterate over those files and cache, read, and munge the data in each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto.s3.connection import S3Connection\n",
    "\n",
    "conn = S3Connection()\n",
    "bucket = conn.get_bucket('earth-data')\n",
    "s3_paths = [f.key for f in bucket.list('carbon_flux/nee_data_fusion/FLX')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for s3_path in s3_paths:\n",
    "    dd = read_and_clean_file(s3_path)\n",
    "    if dd is not None:\n",
    "        datasets.append(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = cat.fluxnet_metadata().read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data\n",
    "\n",
    "Once the data are loaded in, they need to be joined with the metadata relating to each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dask.dataframe.concat(datasets).compute()\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_metadata = pd.get_dummies(metadata, columns=['igbp'])\n",
    "onehot_metadata['igbp'] = metadata['igbp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(X, onehot_metadata, on='site')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show = df.sample(frac=0.10)\n",
    "sites = pd.Categorical(show['site']).codes\n",
    "dropped = {}\n",
    "for col in ['DOY', 'site', 'igbp', 'lat', 'lon']:\n",
    "    dropped[col] = show[col].copy()\n",
    "    show.pop(col)\n",
    "        \n",
    "print(\"{} observations and {} variables\".format(*show.shape))\n",
    "print(\"Generating a prediction with these variables: \\n  {}\".format(\n",
    "    \"\\n  \".join(list(\n",
    "        show.columns\n",
    "    ))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dask\n",
    "With dask, we can distribute tasks over cores and do parallel computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Linear models work well *at one site* but this is confounded by\n",
    "\n",
    "* lat/lon\n",
    "* day of year\n",
    "* environment type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "assert 'site' not in show.columns\n",
    "y = show['y'].values\n",
    "X = pd.DataFrame({col: show[col].values \n",
    "                  for col in show.columns \n",
    "                  if col != 'y'})\n",
    "print(\"X.shape =\", X.shape)\n",
    "assert 'y' not in X.columns\n",
    "\n",
    "# transform data matrix so 0 mean, unit variance for each feature\n",
    "X = StandardScaler().fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fit_and_predict(X_train, y_train, X_test, nbrs=False):\n",
    "    if nbrs:\n",
    "        _nbrs = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(X_train)\n",
    "        distances, indices = _nbrs.kneighbors(X_test)\n",
    "    else:\n",
    "        indices = np.arange(len(X_train), dtype=int)\n",
    "    \n",
    "    X_train_filtered = X_train[indices.flat[:]] \n",
    "    y_train_filtered = y_train[indices.flat[:]] \n",
    "        \n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_filtered, y_train_filtered)\n",
    "    return model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_stats(train_idx, test_idx, X, y, doy=None, predict_each='season', nbrs=False):\n",
    "    start = datetime(2000, 1, 1)\n",
    "    end = start + timedelta(days=365)\n",
    "    \n",
    "    if predict_each == 'month':\n",
    "        get_time_id = lambda dt: dt.month\n",
    "    elif predict_each == 'year':\n",
    "        get_time_id = lambda dt: 1\n",
    "    elif predict_each == 'season':\n",
    "        seasons = {'spring': [3, 4, 5],\n",
    "                   'summer': [6, 7, 8],\n",
    "                   'fall': [9, 10, 11],\n",
    "                   'winter': [12, 1, 2]}\n",
    "        seasons = {month: season_id\n",
    "                   for season_id, months in enumerate(seasons.values())\n",
    "                   for month in months}\n",
    "        get_time_id = lambda dt: seasons[dt.month] \n",
    "    else:\n",
    "        msg = \"predict_each should be in {'year', 'month', 'season'}, got '{}'\"\n",
    "        raise ValueError(msg.format(predict_each))\n",
    "    \n",
    "    # from https://stackoverflow.com/questions/153584/how-to-iterate-over-a-timespan-after-days-hours-weeks-and-months-in-python\n",
    "    time_partitions = {(dt - start).days: get_time_id(dt)\n",
    "                       for dt in rrule.rrule(rrule.DAILY, dtstart=start, until=end)}\n",
    "    time_partitions[366] = max(time_partitions.values())\n",
    "    \n",
    "    test_days = doy[test_idx]\n",
    "    \n",
    "    preds = []\n",
    "    for time_partition in time_partitions.values():\n",
    "        if len(time_partitions.values()) > 1:\n",
    "            time_idx = [i for i, day in enumerate(doy) if time_partitions[day] == time_partition]\n",
    "            \n",
    "            # get the test set specific to this time instance\n",
    "            time_test_idx = np.intersect1d(test_idx, time_idx)\n",
    "        else:\n",
    "            time_test_idx = test_idx \n",
    "\n",
    "        if len(time_test_idx) == 0:\n",
    "            continue\n",
    "            \n",
    "        y_hat = fit_and_predict(X[train_idx], y[train_idx], X[time_test_idx], nbrs=nbrs)\n",
    "        y_test = y[time_test_idx]\n",
    "        preds += [{'predicted': y_hat,\n",
    "                   'actual': y_test,\n",
    "                   'time_partition': time_partition,\n",
    "                   'corrcoef': np.corrcoef(y_hat, y_test)[0][1]}]\n",
    "    actual = [p['actual'] for p in preds]\n",
    "    predicted = [p['predicted'] for p in preds]\n",
    "    actual = np.concatenate(actual).flat[:]\n",
    "    predicted = np.concatenate(predicted).flat[:]\n",
    "    return {'time_partitions': preds,\n",
    "            'actual': actual,\n",
    "            'predicted': predicted,\n",
    "            'corrcoef': np.corrcoef(actual, predicted)[0][1]}\n",
    "\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "sep = LeaveOneGroupOut()\n",
    "train_idx, test_idx = list(sep.split(X, y, sites))[0]\n",
    "_ = prediction_stats(train_idx, test_idx, X, y, doy=dropped['DOY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "sep = LeaveOneGroupOut()\n",
    "corrs = []\n",
    "\n",
    "futures = []\n",
    "n_splits = sep.get_n_splits(X, y, sites)\n",
    "X_future = client.scatter(X)\n",
    "y_future = client.scatter(y)\n",
    "doy_future = client.scatter(dropped['DOY'])\n",
    "for i, (train_index, test_index) in enumerate(sep.split(X, y, sites)):\n",
    "    futures += [{'site_id': i,\n",
    "                 'train_index': train_index,\n",
    "                 'test_index': test_index,\n",
    "                 'stats': client.submit(prediction_stats,\n",
    "                                        train_index,\n",
    "                                        test_index,\n",
    "                                        X_future,\n",
    "                                        y_future,\n",
    "                                        doy=doy_future)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.gather(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [{'site_id': result['site_id'], **result['stats']}\n",
    "       for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(out)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts VLine [show_legend=False] VLine (color='red')\n",
    "corrs = df.corrcoef[~np.isnan(df.corrcoef.values)]\n",
    "frequencies, edges = np.histogram(corrs, 20)\n",
    "\n",
    "c1 = hv.Histogram((frequencies, edges), extents=(-1, None, 1, None))\n",
    "c2 = hv.VLine(np.mean(corrs), label='mean')\n",
    "c3 = hv.VLine(np.median(corrs), label='median')\n",
    "c1 * c2# * c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(corrs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
