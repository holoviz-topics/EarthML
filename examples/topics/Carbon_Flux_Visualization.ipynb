{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carbon Monitoring Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import pandas as pd\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to visualize the data used in the carbon monitoring project [nee_data_fusion](https://github.com/greyNearing/nee_data_fusion/) using Python tools.\n",
    "\n",
    "The goals of this notebook:\n",
    "\n",
    "* examine the measurements from each site\n",
    "* generate some visualization or global model to predict one site from every other site.\n",
    "* generate and explain model idea\n",
    "\n",
    "To run this notebook, you will need `RSIF_2007_2016_05N_01L.mat` in the `examples` directory which you can download from https://gentinelab.eee.columbia.edu/content/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading FluxNet data ``extract_fluxnet.m``\n",
    "\n",
    "[FluxNet](http://fluxnet.fluxdata.org/) is a worldwide collection of sensor stations that record a number of local variables relating to atmospheric conditions, solar flux and soil moisture. The data in the [nee_data_fusion](https://github.com/greyNearing/nee_data_fusion/) repository is expressed as a collection of CSV files where the site names are expressed in the filenames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines functions to\n",
    "\n",
    "* read in the data from all sites\n",
    "* do some data munging (i.e., date parsing, `NaN` replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import intake\n",
    "\n",
    "cat = intake.open_catalog('../catalog.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = ['P_ERA', 'TA_ERA', 'PA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'WS_ERA',\n",
    "        'VPD_ERA', 'SWC_F_MDS_1', 'SWC_F_MDS_2', 'SWC_F_MDS_3',\n",
    "        'TS_F_MDS_1', 'TS_F_MDS_2', 'TS_F_MDS_3', 'TIMESTAMP']\n",
    "\n",
    "train = [*filter(lambda x: x!= 'TIMESTAMP', keep), 'DOY', 'site']\n",
    "\n",
    "def read_and_clean_file(s3_path, predict=\"NEE_CUT_USTAR50\"):\n",
    "    df = cat.fluxnet_daily(s3_path=s3_path).to_dask()\n",
    "    \n",
    "    for col in keep:\n",
    "        if col not in df.columns:\n",
    "            if 'SWC_F' in col or 'TS_F' in col:\n",
    "                df = df.assign(**{col: 0})\n",
    "    \n",
    "    if not (set(df.columns) >= set(keep)) or predict not in df.columns:\n",
    "        print(s3_path, 'is missing required columns')\n",
    "        return\n",
    "\n",
    "    df[keep + [predict]] = df[keep + [predict]].fillna(0)\n",
    "    df = df.assign(DOY=df.TIMESTAMP.dt.dayofyear)\n",
    "\n",
    "    X = df[train]\n",
    "    X = X.assign(y=df[predict])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a helper function to load and clean a particular data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and clean data\n",
    "This will take a few minutes if the data is not cached yet. First we will get a list of all the files on the s3 bucket, then we will iterate over those files and cache, read, and munge the data in each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto.s3.connection import S3Connection\n",
    "\n",
    "conn = S3Connection()\n",
    "bucket = conn.get_bucket('earth-data')\n",
    "s3_paths = [f.key for f in bucket.list('carbon_flux/nee_data_fusion/FLX')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for s3_path in s3_paths:\n",
    "    dd = read_and_clean_file(s3_path)\n",
    "    if dd is not None:\n",
    "        datasets.append(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = cat.fluxnet_metadata().read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data\n",
    "\n",
    "Once the data are loaded in, they need to be joined with the metadata relating to each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dask.dataframe.concat(datasets).compute()\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_metadata = pd.get_dummies(metadata, columns=['igbp'])\n",
    "onehot_metadata['igbp'] = metadata['igbp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(X, onehot_metadata, on='site')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show = df.sample(frac=0.10)\n",
    "sites = pd.Categorical(show['site']).codes\n",
    "dropped = {}\n",
    "for col in ['DOY', 'site', 'igbp', 'lat', 'lon']:\n",
    "    dropped[col] = show[col].copy()\n",
    "    show.pop(col)\n",
    "        \n",
    "print(\"{} observations and {} variables\".format(*show.shape))\n",
    "print(\"Generating a prediction with these variables: \\n  {}\".format(\n",
    "    \"\\n  \".join(list(\n",
    "        show.columns\n",
    "    ))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables are sufficient to create the linear models at every site. However, the site information is hidden from the visualization algorithm.\n",
    "\n",
    "* Good sanity checks:\n",
    "    - latitude encoded some structure, longitude does not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Linear models work well *at one site* but this is confounded by\n",
    "\n",
    "* lat/lon\n",
    "* day of year\n",
    "* environment type\n",
    "\n",
    "We want to generate some visualization that accounts for these 4 variables and helps generate some understanding.\n",
    "\n",
    "That is, these observations lie on some manifold. We want to learn the structure of that manifold, and visualize each observation on that manifold.\n",
    "\n",
    "This work attempts to find similar observations - observations that have a similar structure between the independent variables (e.g., `P_ERA`) and dependent variables (the carbon flux measurement `y`).\n",
    "\n",
    "UMAP is a tool for this, and has firm mathematical grounding (plus, it's nice to use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reduct = umap.UMAP(verbose=True, n_epochs=None)#, n_neighbors=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduct.fit(show.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reduct.embedding_\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['lat', 'lon', 'igbp']\n",
    "s = pd.DataFrame(dropped)\n",
    "s['x0'] = embedding[:, 0]\n",
    "s['x1'] = embedding[:, 1]\n",
    "for col in cols:\n",
    "    if col in show:\n",
    "        s[col] = show[col]\n",
    "    else:\n",
    "        if not col in s:\n",
    "            print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import Select\n",
    "from bokeh.layouts import row, widgetbox\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.plotting import curdoc\n",
    "from holoviews.ipython.display_hooks import display\n",
    "import colorcet as cc\n",
    "\n",
    "colors = ['lat', 'lon', 'DOY', 'site', 'igbp']\n",
    "\n",
    "def create_figure(color='lat', **kwargs):\n",
    "    opts = {'plot': {'color_index': color, 'show_legend': False,\n",
    "                     'width': 600, 'height': 600, 'colorbar': True,\n",
    "                     'tools': ['hover']},\n",
    "            'style': {'cmap': 'magma', 'legend': False}\n",
    "}\n",
    "    if color == 'DOY':\n",
    "        opts['style']['cmap'] = cc.cm['cyclic_mrybm_35_75_c68']\n",
    "    if color == 'igbp':\n",
    "        opts['style']['cmap'] = 'Category20'\n",
    "        opts['plot']['legend_position'] ='right'\n",
    "        opts['plot']['show_legend'] = True\n",
    "    if color == 'site':\n",
    "        opts['style']['cmap'] = 'Category20'\n",
    "        opts['plot']['colorbar'] = False\n",
    "        opts['plot']['width'] = 700\n",
    "\n",
    "    opts.update(**kwargs)\n",
    "    chart = hv.Scatter(\n",
    "        s, kdims=['x0', 'x1'], vdims=[color, 'site'], extents=(-15,-15,15,15)\n",
    "    ).opts(plot=opts['plot'], style=opts['style'])\n",
    "    return display(chart)\n",
    "\n",
    "from ipywidgets import interactive\n",
    "\n",
    "w = interactive(create_figure, color=colors)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a closer look at vegetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igbp_vegetation = {\n",
    "    'ENF': '01 - Evergreen Needleleaf forest',\n",
    "    'EBF': '02 - Evergreen Broadleaf forest',\n",
    "    'DNF': '03 - Deciduous Needleleaf forest',\n",
    "    'DBF': '04 - Deciduous Broadleaf forest',\n",
    "    'MF' : '05 - Mixed forest',\n",
    "    'CSH': '06 - Closed shrublands',\n",
    "    'OSH': '07 - Open shrublands',\n",
    "    'WSA': '08 - Woody savannas',\n",
    "    'SAV': '09 - Savannas',\n",
    "    'GRA': '10 - Grasslands',\n",
    "    'WET': '11 - Permanent wetlands',\n",
    "    'CRO': '12 - Croplands',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['vegetation'] = s['igbp'].apply(lambda x: igbp_vegetation[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = hv.Dataset(s, ['x0', 'vegetation'], ['x1', 'site'])\n",
    "grouped = ds.to(hv.Scatter, kdims=['x0', 'x1'], extents=(-15,-15,15,15), vdims=['site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lpdaac.usgs.gov/about/news_archive/modisterra_land_cover_types_yearly_l3_global_005deg_cmg_mod12c1\n",
    "lpdaac_palette = [\n",
    "    '#008000', '#00FF00', '#99CC00', '#99FF99', '#339966', '#993366',\n",
    "    '#FFCC99', '#CCFFCC', '#FFCC00', '#FF9900', '#006699', '#FFFF00'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Scatter [width=800, height=600] (color=Cycle(lpdaac_palette), size=1, muted_alpha=0)\n",
    "grouped.overlay('vegetation').options(legend_position='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate each vegetation type so that any site eccentricities are made clear. In this, let's **color by site ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.options(color_index='site', cmap='Category20', show_legend=False, size=1, alpha=0.8).layout().cols(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the [Carbon Flux Prediction](Carbon_Flux_Prediction.ipynb) for more info on how to train a predictive model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
