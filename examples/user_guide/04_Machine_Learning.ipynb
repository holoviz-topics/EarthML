{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "With the data preparation complete, this user guide will demonstrate how you can configure a [`scikit-learn`](http://scikit-learn.org/stable/index.html) pipeline. In the next user guide, [Data_Visualization](05_Data_Visualization) you will learn how to visualize the output of this pipeline and diagnose as well as ensure that the inputs to the pipeline have the expected structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "\n",
    "cat = intake.open_catalog('../catalog.yml')\n",
    "l5_da = cat.l8.read_chunked()\n",
    "l5_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting data\n",
    "\n",
    "We'll use a subset of these data in the following examples. There are many ways to [subset data in xarray](http://xarray.pydata.org/en/stable/indexing.html). Here we select a central 2000x2000 pixel piece using index selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l5_da = l5_da[:, 3000:5000, 3000:5000]\n",
    "l5_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l5_da.hvplot(kind='image', x='x', y='y', groupby='band', datashade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll reshape the image to be how dask-ml / scikit-learn expect it: `(n_samples, n_features)` where n_features is the number of bands and n_samples is the total number of pixels in each band. In this case we start with an array that is n_bands by n_y by n_x  (7, 2000, 2000) and we need to reshape to an array that is `(n_samples, n_features)` (4e6, 7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy\n",
    "\n",
    "Data can be reshaped at the lowest level using `numpy`. For instance by getting the values from the `xarray.DataArray`, and using flatten and transpose to get the right shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = l5_da.values\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to flatten along the x and y but not along the band axis, we need to iterate over each band and flatten the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = np.array([arr[i].flatten() for i in range(arr.shape[0])])\n",
    "flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reorder the dimensions using `.transpose`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_by_feature = flattened.transpose()\n",
    "sample_by_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_by_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `numpy.array`s are not labeled data, the semantics of the data are lost over the course of these operations as the necessary metadata does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xarray\n",
    "\n",
    "By using `xarray` methods to flatten the data, we can keep track of the coordiate labels ('x' and 'y') along the way. This means that we have the ability to reshape back to our original array at any time with no information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_by_band = l5_da.stack(z=('x','y'))\n",
    "flattened_by_band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reorder the dimensions using `Dataset.transpose`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_by_feature = flattened_by_band.transpose('z', 'band')\n",
    "sample_by_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data in the shape that we are looking for: a long array of pixels for each band. As a sanity check we can take a look at the plain `np.array`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample_by_feature.values\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other preprocessing\n",
    "\n",
    "Sometimes values are too big or need more axes, or need to have a affine transformation applied. Here we'll demonstrate doing this in `numpy` or `xarray`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add an axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(X, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_by_feature.expand_dims(dim='e', axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rescale:\n",
    "\n",
    "Rescale the data to input to the algorithm since the ML pipeline that we have selected expects input values to be small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X - X.mean()) / X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled = (sample_by_feature - sample_by_feature.mean()) / sample_by_feature.std()\n",
    "rescaled.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Since the the xarray object is in dask, the actual computation isn't performed until `.compute()`is called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ML pipeline\n",
    "The Machine Learning pipeline shown below is just for the purpose of understanding the shaping/reshaping of the data. More on this in the next user guide [Machine_Learning](./04_Machine_Learning.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.cluster import SpectralClustering\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes=False)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute and persist he rescaled data to feed into the ML pipeline. Notice that X has the shape: `n_samples, n_features` as discussed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = client.persist(rescaled)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SpectralClustering(n_clusters=4, random_state=0,\n",
    "                         gamma=None,\n",
    "                         kmeans_params={'init_max_iter': 5},\n",
    "                         persist_embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clf.assign_labels_.labels_.compute()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un-flattening\n",
    "\n",
    "Once the computation is done, the output can used to create a new array with the same structure as the input array. This new output array will have the coordinates needed to be unstacked similarly to how they were stacked. One of the main benefits of using `xarray` for this stacking and unstacking is that allows `xarray` to keep track of the coordinate information for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = sample_by_feature[:, 0]\n",
    "template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Since the original array is n_samples by n_features (4_000_000, 6) and the output only contains one feature (4_000_000,), the template structure for this data needs to have the shape (n_samples). We achieve this by just taking one of the bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_array = template.copy(data=labels)\n",
    "output_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new output array in hand, we can unstack back to the original dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstacked = output_array.unstack()\n",
    "unstacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(unstacked.hvplot(x='x', y='y', datashade=True).relabel('Clustered') + \n",
    " l5_da.sel(band=4).hvplot(x='x', y='y', datashade=True).relabel('Image')\n",
    ").cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic plot\n",
    "\n",
    "The plot above is useful and quick to generate, but it doesn't contain the geo-referencing. Adding the coordinate reference system in the hvplot method, ensures that the data is properly positioned in space. This geo-referencing is made very straightforward because of the way `xarray` persists metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstacked.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = ccrs.epsg(32611)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(unstacked.hvplot(x='x', y='y', datashade=True, crs=crs, height=500).relabel('Clustered') +\n",
    " l5_da.sel(band=3).hvplot.image(x='x', y='y', datashade=True, crs=crs, height=500).relabel('Image')\n",
    ").cols(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
